import unittest
from tokenizer import Tokenizer

class TestTokenizer(unittest.TestCase):
    def __init__(self, *args, **kwargs):
        super(TestTokenizer, self).__init__(*args, **kwargs)
        self.vocab_path = "vocab.json"
        self.merges_path = "merges.txt"
    def test_load_file(self):
        tokenizer = Tokenizer()
        tokenizer.load_vocab_and_merges(self.vocab_path, self.merges_path)
        self.assertEqual(tokenizer, tokenizer)  # add assertion here

    def test_tokenize(self):
        tokenizer = Tokenizer()
        tokenizer.load_vocab_and_merges(self.vocab_path, self.merges_path)
        data = 'ffd8ffe000104a46494600010101004800480000ffdb0043000302020302020303030304030304050805050404050a070706080c0a0c0c0b0a0b0b0d0e12100d0e110e0b0b1016101113141515150c0f171816141812141514ffdb00430103040405040509050509140d0b0d1414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414ffc000110801e0028003011100021101031101ffc4001d000000070101010000000000000000000001020304050607080009ffc4005b10000201020502040304060606080105110102030411000506122131410713225114617108328191152342a1b1c116245262d1f00933728292e117253443a2b2c2f1531826354463a3d2195455647374838493b3c3d47685d3ffc4001c0100020301010101000000000000000000000102030405060708ffc4004411000103020304080502050401030403010100021103210412310541516113718191a1b1d1f0142232c1e1064215233352f1627292a2822443b2'
        self.assertEqual(data, tokenizer.decode(tokenizer.encode(data)))

    def test_tokenize_v2(self):
        tokenizer = Tokenizer()
        tokenizer.load_vocab_and_merges(self.vocab_path, self.merges_path)
        data = 'ffd8ffe000104a4649460001010100480048000db004300030200302020303030304030304050805050404050a070706080c0a0c0c0b00b0b0d0e12100d0e110e0b0b1016101113141515150c0f1718161341812141514ffdb004301030405040509050509140d0b0d1414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414ffc000110801e0028003011100021101031101ffc4001d000000070101010000000000000000000001020304050607080009ffc4005b10000201020502040304060606080105110102030411000506122131410713225114617108328191152342a1b1c116245262d1f00933728292e117253443a2b2c2f1531826354463a3d2195455647374838493b3c3d47685d3ffc4001c0100020301010101000000000000000000000102030405060708ffc4004411000103020304080502050401030403010100021103210412310541516113718191a1b1d1f0142232c1e1064215233352f1627292a2822443b2'
        self.assertEqual(data, tokenizer.decode(tokenizer.encode(data)))


if __name__ == '__main__':
    unittest.main()
