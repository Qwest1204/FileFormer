{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T06:59:51.056115Z",
     "start_time": "2025-05-31T06:59:51.053989Z"
    }
   },
   "cell_type": "code",
   "source": "from pathlib import Path",
   "id": "5b7837413df762fd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-31T06:59:51.289041Z",
     "start_time": "2025-05-31T06:59:51.280177Z"
    }
   },
   "source": "files = [x for x in Path(\"/Users/daniilogorodnikov/dataset/app\").glob('**/*') if x.is_file()]",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T06:59:51.351003Z",
     "start_time": "2025-05-31T06:59:51.349449Z"
    }
   },
   "cell_type": "code",
   "source": "bytes = []",
   "id": "4f623e15ff3c53be",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T06:59:52.114442Z",
     "start_time": "2025-05-31T06:59:51.549222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in files:\n",
    "    with open(file, 'rb') as f:\n",
    "        data_from_file = f.read().hex()\n",
    "        bytes.append(data_from_file)\n",
    "\n"
   ],
   "id": "ad1bbd851385fbb0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T07:34:57.467651Z",
     "start_time": "2025-05-31T07:34:57.443267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with 'Ġ'\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including 'Ġ' if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # Extend unique_chars with characters from processed_text that are not already included\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "\n",
    "        # Optionally, ensure 'Ġ' is included if it is relevant to your text processing\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "\n",
    "        # Now create the vocab and inverse vocab dictionaries\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:  # No more pairs to merge. Stopping training.\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained vocabulary and BPE merges from OpenAI's GPT-2 files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json').\n",
    "            bpe_merges_path (str): Path to the bpe_merges file  (GPT-2 calls it 'vocab.bpe').\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # loaded_vocab maps token_str to token_id\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}  # token_id: token_str\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}  # token_str: token_id\n",
    "\n",
    "        # Load BPE merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            # Skip header line if present\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            for rank, line in enumerate(lines):\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) != 2:\n",
    "                    print(f\"Line {rank+1} has more than 2 entries: {line.strip()}\")\n",
    "                    continue\n",
    "                token1, token2 = pair\n",
    "                if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                    token_id1 = self.inverse_vocab[token1]\n",
    "                    token_id2 = self.inverse_vocab[token2]\n",
    "                    merged_token = token1 + token2\n",
    "                    if merged_token in self.inverse_vocab:\n",
    "                        merged_token_id = self.inverse_vocab[merged_token]\n",
    "                        self.bpe_merges[(token_id1, token_id2)] = merged_token_id\n",
    "                        # print(f\"Loaded merge: '{token1}' + '{token2}' -> '{merged_token}' (ID: {merged_token_id})\")\n",
    "                    else:\n",
    "                        print(f\"Merged token '{merged_token}' not found in vocab. Skipping.\")\n",
    "                else:\n",
    "                    print(f\"Skipping pair {pair} as one of the tokens is not in the vocabulary.\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to encode.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # Split text into tokens, keeping newlines intact\n",
    "        words = text.replace(\"\\n\", \" \\n \").split()  # Ensure '\\n' is treated as a separate token\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and not word.startswith(\"\\n\"):\n",
    "                tokens.append(\"Ġ\" + word)  # Add 'Ġ' to words that follow a space or newline\n",
    "            else:\n",
    "                tokens.append(word)  # Handle first word or standalone '\\n'\n",
    "\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                # token is contained in the vocabulary as is\n",
    "                token_id = self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                # Attempt to handle subword tokenization via BPE\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    # Uncomment for educational purposes:\n",
    "                    # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                    i += 2  # Skip the next token as it's merged\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token.startswith(\"Ġ\"):\n",
    "                # Replace 'Ġ' with a space\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Save the vocabulary and BPE merges to JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to save the vocabulary.\n",
    "            bpe_merges_path (str): Path to save the BPE merges.\n",
    "        \"\"\"\n",
    "        # Save vocabulary\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump({k: v for k, v in self.vocab.items()}, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Save BPE merges as a list of dictionaries\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load the vocabulary and BPE merges from JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocabulary file.\n",
    "            bpe_merges_path (str): Path to the BPE merges file.\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Load BPE merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge['pair'])\n",
    "                new_id = merge['new_id']\n",
    "                self.bpe_merges[pair] = new_id\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ],
   "id": "decabbc53a63dadf",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T07:35:22.203874Z",
     "start_time": "2025-05-31T07:35:22.199633Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = BPETokenizerSimple()",
   "id": "3113cb6ae8bb8a1a",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T20:39:48.096204Z",
     "start_time": "2025-05-31T20:38:51.787577Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.train(bytes[0], vocab_size=12880, allowed_special=\"<|endoffile|>\")",
   "id": "abab3739c1a36e98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x104aa92b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daniilogorodnikov/PycharmProjects/Notus/.venv/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[65]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mbytes\u001B[39;49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m12880\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowed_special\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m<|endoffile|>\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 65\u001B[39m, in \u001B[36mBPETokenizerSimple.train\u001B[39m\u001B[34m(self, text, vocab_size, allowed_special)\u001B[39m\n\u001B[32m     63\u001B[39m \u001B[38;5;66;03m# BPE steps 1-3: Repeatedly find and replace frequent pairs\u001B[39;00m\n\u001B[32m     64\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m new_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.vocab), vocab_size):\n\u001B[32m---> \u001B[39m\u001B[32m65\u001B[39m     pair_id = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfind_freq_pair\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmost\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     66\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m pair_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# No more pairs to merge. Stopping training.\u001B[39;00m\n\u001B[32m     67\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 258\u001B[39m, in \u001B[36mBPETokenizerSimple.find_freq_pair\u001B[39m\u001B[34m(token_ids, mode)\u001B[39m\n\u001B[32m    256\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m    257\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfind_freq_pair\u001B[39m(token_ids, mode=\u001B[33m\"\u001B[39m\u001B[33mmost\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m258\u001B[39m     pairs = \u001B[43mCounter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    260\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m mode == \u001B[33m\"\u001B[39m\u001B[33mmost\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    261\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(pairs.items(), key=\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[32m1\u001B[39m])[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/collections/__init__.py:608\u001B[39m, in \u001B[36mCounter.__init__\u001B[39m\u001B[34m(self, iterable, **kwds)\u001B[39m\n\u001B[32m    597\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m'''Create a new, empty Counter object.  And if given, count elements\u001B[39;00m\n\u001B[32m    598\u001B[39m \u001B[33;03mfrom an input iterable.  Or, initialize the count from another mapping\u001B[39;00m\n\u001B[32m    599\u001B[39m \u001B[33;03mof elements to their counts.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    605\u001B[39m \n\u001B[32m    606\u001B[39m \u001B[33;03m'''\u001B[39;00m\n\u001B[32m    607\u001B[39m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m()\n\u001B[32m--> \u001B[39m\u001B[32m608\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/collections/__init__.py:700\u001B[39m, in \u001B[36mCounter.update\u001B[39m\u001B[34m(self, iterable, **kwds)\u001B[39m\n\u001B[32m    698\u001B[39m             \u001B[38;5;28msuper\u001B[39m().update(iterable)\n\u001B[32m    699\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m700\u001B[39m         \u001B[43m_count_elements\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    701\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwds:\n\u001B[32m    702\u001B[39m     \u001B[38;5;28mself\u001B[39m.update(kwds)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T19:39:19.180916Z",
     "start_time": "2025-05-31T19:39:19.179058Z"
    }
   },
   "cell_type": "code",
   "source": "tokens = tokenizer.encode(bytes[1][:100])",
   "id": "c2b513046c30dd63",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T19:40:01.906043Z",
     "start_time": "2025-05-31T19:40:01.898868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for token_id in tokens:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ],
   "id": "8d01d34ce7a616d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518 -> ff\n",
      "378 -> d8\n",
      "518 -> ff\n",
      "416 -> e0\n",
      "257 -> 00\n",
      "319 -> 10\n",
      "501 -> 4a\n",
      "482 -> 46\n",
      "489 -> 49\n",
      "482 -> 46\n",
      "2446 -> 0001\n",
      "460 -> 01\n",
      "460 -> 01\n",
      "460 -> 01\n",
      "268 -> 2c\n",
      "460 -> 01\n",
      "2277 -> 2c00\n",
      "257 -> 00\n",
      "518 -> ff\n",
      "475 -> e2\n",
      "500 -> 02\n",
      "314 -> 84\n",
      "489 -> 49\n",
      "504 -> 43\n",
      "504 -> 43\n",
      "357 -> 5f\n",
      "361 -> 50\n",
      "473 -> 52\n",
      "333 -> 4f\n",
      "482 -> 46\n",
      "489 -> 49\n",
      "432 -> 4c\n",
      "436 -> 45\n",
      "2446 -> 0001\n",
      "460 -> 01\n",
      "897 -> 0000\n",
      "500 -> 02\n",
      "353 -> 74\n",
      "496 -> 41\n",
      "2842 -> 5050\n",
      "432 -> 4c\n",
      "500 -> 02\n",
      "281 -> 20\n",
      "897 -> 0000\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T19:40:22.735626Z",
     "start_time": "2025-05-31T19:40:22.731908Z"
    }
   },
   "cell_type": "code",
   "source": "bytes[1][:100]\n",
   "id": "270ca0e4f4f1149",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ffd8ffe000104a46494600010101012c012c0000ffe202844943435f50524f46494c45000101000002744150504c02200000'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T19:41:41.059409Z",
     "start_time": "2025-05-31T19:41:41.046595Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.vocab",
   "id": "5829ac78ae0977a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\x00',\n",
       " 1: '\\x01',\n",
       " 2: '\\x02',\n",
       " 3: '\\x03',\n",
       " 4: '\\x04',\n",
       " 5: '\\x05',\n",
       " 6: '\\x06',\n",
       " 7: '\\x07',\n",
       " 8: '\\x08',\n",
       " 9: '\\t',\n",
       " 10: '\\n',\n",
       " 11: '\\x0b',\n",
       " 12: '\\x0c',\n",
       " 13: '\\r',\n",
       " 14: '\\x0e',\n",
       " 15: '\\x0f',\n",
       " 16: '\\x10',\n",
       " 17: '\\x11',\n",
       " 18: '\\x12',\n",
       " 19: '\\x13',\n",
       " 20: '\\x14',\n",
       " 21: '\\x15',\n",
       " 22: '\\x16',\n",
       " 23: '\\x17',\n",
       " 24: '\\x18',\n",
       " 25: '\\x19',\n",
       " 26: '\\x1a',\n",
       " 27: '\\x1b',\n",
       " 28: '\\x1c',\n",
       " 29: '\\x1d',\n",
       " 30: '\\x1e',\n",
       " 31: '\\x1f',\n",
       " 32: ' ',\n",
       " 33: '!',\n",
       " 34: '\"',\n",
       " 35: '#',\n",
       " 36: '$',\n",
       " 37: '%',\n",
       " 38: '&',\n",
       " 39: \"'\",\n",
       " 40: '(',\n",
       " 41: ')',\n",
       " 42: '*',\n",
       " 43: '+',\n",
       " 44: ',',\n",
       " 45: '-',\n",
       " 46: '.',\n",
       " 47: '/',\n",
       " 48: '0',\n",
       " 49: '1',\n",
       " 50: '2',\n",
       " 51: '3',\n",
       " 52: '4',\n",
       " 53: '5',\n",
       " 54: '6',\n",
       " 55: '7',\n",
       " 56: '8',\n",
       " 57: '9',\n",
       " 58: ':',\n",
       " 59: ';',\n",
       " 60: '<',\n",
       " 61: '=',\n",
       " 62: '>',\n",
       " 63: '?',\n",
       " 64: '@',\n",
       " 65: 'A',\n",
       " 66: 'B',\n",
       " 67: 'C',\n",
       " 68: 'D',\n",
       " 69: 'E',\n",
       " 70: 'F',\n",
       " 71: 'G',\n",
       " 72: 'H',\n",
       " 73: 'I',\n",
       " 74: 'J',\n",
       " 75: 'K',\n",
       " 76: 'L',\n",
       " 77: 'M',\n",
       " 78: 'N',\n",
       " 79: 'O',\n",
       " 80: 'P',\n",
       " 81: 'Q',\n",
       " 82: 'R',\n",
       " 83: 'S',\n",
       " 84: 'T',\n",
       " 85: 'U',\n",
       " 86: 'V',\n",
       " 87: 'W',\n",
       " 88: 'X',\n",
       " 89: 'Y',\n",
       " 90: 'Z',\n",
       " 91: '[',\n",
       " 92: '\\\\',\n",
       " 93: ']',\n",
       " 94: '^',\n",
       " 95: '_',\n",
       " 96: '`',\n",
       " 97: 'a',\n",
       " 98: 'b',\n",
       " 99: 'c',\n",
       " 100: 'd',\n",
       " 101: 'e',\n",
       " 102: 'f',\n",
       " 103: 'g',\n",
       " 104: 'h',\n",
       " 105: 'i',\n",
       " 106: 'j',\n",
       " 107: 'k',\n",
       " 108: 'l',\n",
       " 109: 'm',\n",
       " 110: 'n',\n",
       " 111: 'o',\n",
       " 112: 'p',\n",
       " 113: 'q',\n",
       " 114: 'r',\n",
       " 115: 's',\n",
       " 116: 't',\n",
       " 117: 'u',\n",
       " 118: 'v',\n",
       " 119: 'w',\n",
       " 120: 'x',\n",
       " 121: 'y',\n",
       " 122: 'z',\n",
       " 123: '{',\n",
       " 124: '|',\n",
       " 125: '}',\n",
       " 126: '~',\n",
       " 127: '\\x7f',\n",
       " 128: '\\x80',\n",
       " 129: '\\x81',\n",
       " 130: '\\x82',\n",
       " 131: '\\x83',\n",
       " 132: '\\x84',\n",
       " 133: '\\x85',\n",
       " 134: '\\x86',\n",
       " 135: '\\x87',\n",
       " 136: '\\x88',\n",
       " 137: '\\x89',\n",
       " 138: '\\x8a',\n",
       " 139: '\\x8b',\n",
       " 140: '\\x8c',\n",
       " 141: '\\x8d',\n",
       " 142: '\\x8e',\n",
       " 143: '\\x8f',\n",
       " 144: '\\x90',\n",
       " 145: '\\x91',\n",
       " 146: '\\x92',\n",
       " 147: '\\x93',\n",
       " 148: '\\x94',\n",
       " 149: '\\x95',\n",
       " 150: '\\x96',\n",
       " 151: '\\x97',\n",
       " 152: '\\x98',\n",
       " 153: '\\x99',\n",
       " 154: '\\x9a',\n",
       " 155: '\\x9b',\n",
       " 156: '\\x9c',\n",
       " 157: '\\x9d',\n",
       " 158: '\\x9e',\n",
       " 159: '\\x9f',\n",
       " 160: '\\xa0',\n",
       " 161: '¡',\n",
       " 162: '¢',\n",
       " 163: '£',\n",
       " 164: '¤',\n",
       " 165: '¥',\n",
       " 166: '¦',\n",
       " 167: '§',\n",
       " 168: '¨',\n",
       " 169: '©',\n",
       " 170: 'ª',\n",
       " 171: '«',\n",
       " 172: '¬',\n",
       " 173: '\\xad',\n",
       " 174: '®',\n",
       " 175: '¯',\n",
       " 176: '°',\n",
       " 177: '±',\n",
       " 178: '²',\n",
       " 179: '³',\n",
       " 180: '´',\n",
       " 181: 'µ',\n",
       " 182: '¶',\n",
       " 183: '·',\n",
       " 184: '¸',\n",
       " 185: '¹',\n",
       " 186: 'º',\n",
       " 187: '»',\n",
       " 188: '¼',\n",
       " 189: '½',\n",
       " 190: '¾',\n",
       " 191: '¿',\n",
       " 192: 'À',\n",
       " 193: 'Á',\n",
       " 194: 'Â',\n",
       " 195: 'Ã',\n",
       " 196: 'Ä',\n",
       " 197: 'Å',\n",
       " 198: 'Æ',\n",
       " 199: 'Ç',\n",
       " 200: 'È',\n",
       " 201: 'É',\n",
       " 202: 'Ê',\n",
       " 203: 'Ë',\n",
       " 204: 'Ì',\n",
       " 205: 'Í',\n",
       " 206: 'Î',\n",
       " 207: 'Ï',\n",
       " 208: 'Ð',\n",
       " 209: 'Ñ',\n",
       " 210: 'Ò',\n",
       " 211: 'Ó',\n",
       " 212: 'Ô',\n",
       " 213: 'Õ',\n",
       " 214: 'Ö',\n",
       " 215: '×',\n",
       " 216: 'Ø',\n",
       " 217: 'Ù',\n",
       " 218: 'Ú',\n",
       " 219: 'Û',\n",
       " 220: 'Ü',\n",
       " 221: 'Ý',\n",
       " 222: 'Þ',\n",
       " 223: 'ß',\n",
       " 224: 'à',\n",
       " 225: 'á',\n",
       " 226: 'â',\n",
       " 227: 'ã',\n",
       " 228: 'ä',\n",
       " 229: 'å',\n",
       " 230: 'æ',\n",
       " 231: 'ç',\n",
       " 232: 'è',\n",
       " 233: 'é',\n",
       " 234: 'ê',\n",
       " 235: 'ë',\n",
       " 236: 'ì',\n",
       " 237: 'í',\n",
       " 238: 'î',\n",
       " 239: 'ï',\n",
       " 240: 'ð',\n",
       " 241: 'ñ',\n",
       " 242: 'ò',\n",
       " 243: 'ó',\n",
       " 244: 'ô',\n",
       " 245: 'õ',\n",
       " 246: 'ö',\n",
       " 247: '÷',\n",
       " 248: 'ø',\n",
       " 249: 'ù',\n",
       " 250: 'ú',\n",
       " 251: 'û',\n",
       " 252: 'ü',\n",
       " 253: 'ý',\n",
       " 254: 'þ',\n",
       " 255: 'ÿ',\n",
       " 256: 'Ġ',\n",
       " 257: '00',\n",
       " 258: '24',\n",
       " 259: '55',\n",
       " 260: '8e',\n",
       " 261: '6e',\n",
       " 262: 'aa',\n",
       " 263: 'dc',\n",
       " 264: '9e',\n",
       " 265: 'd4',\n",
       " 266: '3c',\n",
       " 267: '54',\n",
       " 268: '2c',\n",
       " 269: '37',\n",
       " 270: '2a',\n",
       " 271: '9b',\n",
       " 272: '91',\n",
       " 273: '6b',\n",
       " 274: 'db',\n",
       " 275: '8b',\n",
       " 276: '94',\n",
       " 277: '1b',\n",
       " 278: 'df',\n",
       " 279: '8f',\n",
       " 280: '1c',\n",
       " 281: '20',\n",
       " 282: '6f',\n",
       " 283: 'a7',\n",
       " 284: '6c',\n",
       " 285: '34',\n",
       " 286: '95',\n",
       " 287: '14',\n",
       " 288: '90',\n",
       " 289: 'a4',\n",
       " 290: 'd7',\n",
       " 291: 'de',\n",
       " 292: '2e',\n",
       " 293: '1e',\n",
       " 294: '17',\n",
       " 295: '80',\n",
       " 296: '3b',\n",
       " 297: '65',\n",
       " 298: '2b',\n",
       " 299: 'd5',\n",
       " 300: '27',\n",
       " 301: '8c',\n",
       " 302: '97',\n",
       " 303: '64',\n",
       " 304: '2f',\n",
       " 305: '3f',\n",
       " 306: '3e',\n",
       " 307: '9f',\n",
       " 308: '30',\n",
       " 309: '85',\n",
       " 310: 'ac',\n",
       " 311: '1f',\n",
       " 312: '87',\n",
       " 313: '25',\n",
       " 314: '84',\n",
       " 315: '9c',\n",
       " 316: '35',\n",
       " 317: '60',\n",
       " 318: 'ab',\n",
       " 319: '10',\n",
       " 320: 'a5',\n",
       " 321: 'af',\n",
       " 322: 'ae',\n",
       " 323: '15',\n",
       " 324: '67',\n",
       " 325: 'a0',\n",
       " 326: 'd0',\n",
       " 327: '96',\n",
       " 328: '2d',\n",
       " 329: '23',\n",
       " 330: '66',\n",
       " 331: '7e',\n",
       " 332: 'd3',\n",
       " 333: '4f',\n",
       " 334: '7b',\n",
       " 335: 'a8',\n",
       " 336: '5c',\n",
       " 337: '5b',\n",
       " 338: '16',\n",
       " 339: 'cb',\n",
       " 340: '0b',\n",
       " 341: '93',\n",
       " 342: '8d',\n",
       " 343: '92',\n",
       " 344: '0e',\n",
       " 345: '86',\n",
       " 346: '7f',\n",
       " 347: 'a6',\n",
       " 348: '5e',\n",
       " 349: 'f00',\n",
       " 350: 'c4',\n",
       " 351: 'a3',\n",
       " 352: 'ad',\n",
       " 353: '74',\n",
       " 354: 'cf',\n",
       " 355: '11',\n",
       " 356: '83',\n",
       " 357: '5f',\n",
       " 358: '70',\n",
       " 359: 'a9',\n",
       " 360: '12',\n",
       " 361: '50',\n",
       " 362: '13',\n",
       " 363: '4b',\n",
       " 364: 'cc',\n",
       " 365: '1d',\n",
       " 366: 'ee',\n",
       " 367: '0f',\n",
       " 368: '18',\n",
       " 369: '98',\n",
       " 370: '6d',\n",
       " 371: '28',\n",
       " 372: '63',\n",
       " 373: 'c7',\n",
       " 374: '29',\n",
       " 375: '04',\n",
       " 376: 'bf',\n",
       " 377: '33',\n",
       " 378: 'd8',\n",
       " 379: '26',\n",
       " 380: '99',\n",
       " 381: 'ef',\n",
       " 382: 'b4',\n",
       " 383: '75',\n",
       " 384: '7c',\n",
       " 385: 'a1',\n",
       " 386: '68',\n",
       " 387: 'be',\n",
       " 388: '21',\n",
       " 389: '9d',\n",
       " 390: '77',\n",
       " 391: '36',\n",
       " 392: 'c0',\n",
       " 393: 'e4',\n",
       " 394: '3d',\n",
       " 395: 'a2',\n",
       " 396: '38',\n",
       " 397: 'f4',\n",
       " 398: 'ce',\n",
       " 399: '58',\n",
       " 400: '47',\n",
       " 401: 'fe',\n",
       " 402: 'd6',\n",
       " 403: 'd1',\n",
       " 404: '88',\n",
       " 405: '9a',\n",
       " 406: 'bb',\n",
       " 407: 'c5',\n",
       " 408: '44',\n",
       " 409: '31',\n",
       " 410: '22',\n",
       " 411: 'b7',\n",
       " 412: '57',\n",
       " 413: 'b0',\n",
       " 414: 'd2',\n",
       " 415: 'e7',\n",
       " 416: 'e0',\n",
       " 417: 'f0',\n",
       " 418: '81',\n",
       " 419: 'b5',\n",
       " 420: '62',\n",
       " 421: '4e',\n",
       " 422: 'dd',\n",
       " 423: '82',\n",
       " 424: '61',\n",
       " 425: '32',\n",
       " 426: '6a',\n",
       " 427: '07',\n",
       " 428: 'fc',\n",
       " 429: '19',\n",
       " 430: '69',\n",
       " 431: 'e5',\n",
       " 432: '4c',\n",
       " 433: 'fb',\n",
       " 434: 'f5',\n",
       " 435: 'd9',\n",
       " 436: '45',\n",
       " 437: '39',\n",
       " 438: '8a',\n",
       " 439: 'eb',\n",
       " 440: '89',\n",
       " 441: '0c',\n",
       " 442: 'da',\n",
       " 443: '05',\n",
       " 444: 'bc',\n",
       " 445: '1a',\n",
       " 446: '71',\n",
       " 447: '3a',\n",
       " 448: '78',\n",
       " 449: 'f7',\n",
       " 450: 'ec',\n",
       " 451: '40',\n",
       " 452: 'ff00',\n",
       " 453: '48',\n",
       " 454: 'cd',\n",
       " 455: 'e3',\n",
       " 456: 'f1',\n",
       " 457: '73',\n",
       " 458: 'c3',\n",
       " 459: '4d',\n",
       " 460: '01',\n",
       " 461: '76',\n",
       " 462: '53',\n",
       " 463: '7a',\n",
       " 464: 'fd',\n",
       " 465: 'b1',\n",
       " 466: '72',\n",
       " 467: '51',\n",
       " 468: '79',\n",
       " 469: 'f8',\n",
       " 470: '7d',\n",
       " 471: 'ed',\n",
       " 472: 'ea',\n",
       " 473: '52',\n",
       " 474: 'b8',\n",
       " 475: 'e2',\n",
       " 476: '924',\n",
       " 477: 'e6',\n",
       " 478: '03',\n",
       " 479: 'e1',\n",
       " 480: 'f3',\n",
       " 481: 'f2',\n",
       " 482: '46',\n",
       " 483: 'b2',\n",
       " 484: 'b6',\n",
       " 485: 'ca',\n",
       " 486: 'bd',\n",
       " 487: '59',\n",
       " 488: '08',\n",
       " 489: '49',\n",
       " 490: 'c1',\n",
       " 491: '5d',\n",
       " 492: 'e8',\n",
       " 493: 'c6',\n",
       " 494: 'b3',\n",
       " 495: '56',\n",
       " 496: '41',\n",
       " 497: 'c9',\n",
       " 498: '42',\n",
       " 499: 'ba',\n",
       " 500: '02',\n",
       " 501: '4a',\n",
       " 502: 'c8',\n",
       " 503: '0d',\n",
       " 504: '43',\n",
       " 505: 'f9',\n",
       " 506: 'f6',\n",
       " 507: 'e9',\n",
       " 508: '06',\n",
       " 509: 'c2',\n",
       " 510: 'fa',\n",
       " 511: '92a',\n",
       " 512: '5a',\n",
       " 513: '000',\n",
       " 514: '6e7',\n",
       " 515: 'b9',\n",
       " 516: '0a',\n",
       " 517: 'dc7',\n",
       " 518: 'ff',\n",
       " 519: '124',\n",
       " 520: '920',\n",
       " 521: '93c',\n",
       " 522: '37e',\n",
       " 523: '904',\n",
       " 524: '83c',\n",
       " 525: '09',\n",
       " 526: '6c4',\n",
       " 527: '8bf',\n",
       " 528: '6e4',\n",
       " 529: 'a9e',\n",
       " 530: '22c',\n",
       " 531: '8dc',\n",
       " 532: '16b',\n",
       " 533: '137',\n",
       " 534: 'd7b',\n",
       " 535: '8e0',\n",
       " 536: '6e0',\n",
       " 537: '964',\n",
       " 538: '804',\n",
       " 539: 'd37',\n",
       " 540: '8e4',\n",
       " 541: '2dc',\n",
       " 542: '955',\n",
       " 543: '2aa',\n",
       " 544: '6dc',\n",
       " 545: '3d4',\n",
       " 546: '1cf',\n",
       " 547: '800',\n",
       " 548: '91c',\n",
       " 549: '9aa',\n",
       " 550: 'a9b',\n",
       " 551: '95b',\n",
       " 552: '544',\n",
       " 553: 'd4f',\n",
       " 554: '9e0',\n",
       " 555: '994',\n",
       " 556: '6bf',\n",
       " 557: '824',\n",
       " 558: '8eb',\n",
       " 559: '637',\n",
       " 560: '1bf',\n",
       " 561: '17b',\n",
       " 562: 'dfb',\n",
       " 563: '900',\n",
       " 564: '12a',\n",
       " 565: 'd3b',\n",
       " 566: 'd55',\n",
       " 567: '655',\n",
       " 568: '2d7',\n",
       " 569: '8a4',\n",
       " 570: 'adc',\n",
       " 571: 'd4b',\n",
       " 572: '837',\n",
       " 573: 'a2a',\n",
       " 574: '2c4',\n",
       " 575: '100',\n",
       " 576: '1414',\n",
       " 577: '927',\n",
       " 578: '3c7',\n",
       " 579: '9dc',\n",
       " 580: 'd8f',\n",
       " 581: '724',\n",
       " 582: 'd57',\n",
       " 583: '91f',\n",
       " 584: '6eb',\n",
       " 585: 'a95',\n",
       " 586: 'd87',\n",
       " 587: 'd24',\n",
       " 588: '6e5',\n",
       " 589: '894',\n",
       " 590: '820',\n",
       " 591: 'a47',\n",
       " 592: '92c',\n",
       " 593: '9c4',\n",
       " 594: 'dec',\n",
       " 595: 'a54',\n",
       " 596: 'a8e',\n",
       " 597: '2a4',\n",
       " 598: '6b7',\n",
       " 599: '11b',\n",
       " 600: '9e4',\n",
       " 601: '91b',\n",
       " 602: '3cf',\n",
       " 603: '965',\n",
       " 604: '117',\n",
       " 605: '98e',\n",
       " 606: '604',\n",
       " 607: 'dc0',\n",
       " 608: 'd7e',\n",
       " 609: '624',\n",
       " 610: 'adf',\n",
       " 611: '937',\n",
       " 612: 'a5f',\n",
       " 613: '2e4',\n",
       " 614: '8aa',\n",
       " 615: '9a4',\n",
       " 616: 'a7f',\n",
       " 617: '9b7',\n",
       " 618: '600',\n",
       " 619: '995',\n",
       " 620: '6c0',\n",
       " 621: '254',\n",
       " 622: 'def',\n",
       " 623: 'a9c',\n",
       " 624: '3cc',\n",
       " 625: 'dae',\n",
       " 626: '2d4',\n",
       " 627: '8b7',\n",
       " 628: '80b',\n",
       " 629: '20f',\n",
       " 630: '85f',\n",
       " 631: '237',\n",
       " 632: 'adb',\n",
       " 633: '547',\n",
       " 634: '2ac',\n",
       " 635: 'a8f',\n",
       " 636: '54c',\n",
       " 637: 'd3f',\n",
       " 638: 'a55',\n",
       " 639: 'a37',\n",
       " 640: '974',\n",
       " 641: '9bf',\n",
       " 642: 'd2b',\n",
       " 643: '290',\n",
       " 644: '6f4',\n",
       " 645: 'd54',\n",
       " 646: '355',\n",
       " 647: '1b7',\n",
       " 648: '3ef',\n",
       " 649: '31c',\n",
       " 650: '94c',\n",
       " 651: '99c',\n",
       " 652: '2ab',\n",
       " 653: '155',\n",
       " 654: '224',\n",
       " 655: '3d7',\n",
       " 656: '2db',\n",
       " 657: '9eb',\n",
       " 658: '29b',\n",
       " 659: '9e3',\n",
       " 660: '8de',\n",
       " 661: 'dcf',\n",
       " 662: '925',\n",
       " 663: 'd85',\n",
       " 664: '61c',\n",
       " 665: 'd80',\n",
       " 666: '6db',\n",
       " 667: '2fc',\n",
       " 668: '91e',\n",
       " 669: '587',\n",
       " 670: '68f',\n",
       " 671: '58f',\n",
       " 672: '7bf',\n",
       " 673: 'd47',\n",
       " 674: '960',\n",
       " 675: 'a8c',\n",
       " 676: '657',\n",
       " 677: '6ef',\n",
       " 678: '17e',\n",
       " 679: '6ab',\n",
       " 680: '63c',\n",
       " 681: '207',\n",
       " 682: '255',\n",
       " 683: 'a8b',\n",
       " 684: 'df7',\n",
       " 685: '284',\n",
       " 686: '534',\n",
       " 687: '32b',\n",
       " 688: 'a24',\n",
       " 689: '3aa',\n",
       " 690: '16f',\n",
       " 691: 'daf',\n",
       " 692: '3e7',\n",
       " 693: '690',\n",
       " 694: '975',\n",
       " 695: '104',\n",
       " 696: '234',\n",
       " 697: '291',\n",
       " 698: '300',\n",
       " 699: 'a90',\n",
       " 700: '127',\n",
       " 701: '1aa',\n",
       " 702: '81e',\n",
       " 703: '6cc',\n",
       " 704: 'd27',\n",
       " 705: '200',\n",
       " 706: '28e',\n",
       " 707: 'a80',\n",
       " 708: '36e',\n",
       " 709: 'd34',\n",
       " 710: '827',\n",
       " 711: '8ab',\n",
       " 712: '11c',\n",
       " 713: '54f',\n",
       " 714: '8d4',\n",
       " 715: '22a',\n",
       " 716: 'dbf',\n",
       " 717: 'a00',\n",
       " 718: '907',\n",
       " 719: '324',\n",
       " 720: 'dac',\n",
       " 721: '617',\n",
       " 722: '99e',\n",
       " 723: '23f',\n",
       " 724: 'a2f',\n",
       " 725: '1b4',\n",
       " 726: '2e7',\n",
       " 727: '1dc',\n",
       " 728: '93f',\n",
       " 729: 'a6e',\n",
       " 730: 'a77',\n",
       " 731: '96e',\n",
       " 732: '6b5',\n",
       " 733: '65f',\n",
       " 734: '69e',\n",
       " 735: '627',\n",
       " 736: '22f',\n",
       " 737: '38b',\n",
       " 738: 'd1b',\n",
       " 739: '3af',\n",
       " 740: '6fc',\n",
       " 741: '954',\n",
       " 742: '625',\n",
       " 743: 'd7f',\n",
       " 744: '81f',\n",
       " 745: '81c',\n",
       " 746: 'a27',\n",
       " 747: '6f7',\n",
       " 748: '914',\n",
       " 749: '350',\n",
       " 750: '32a',\n",
       " 751: '675',\n",
       " 752: 'da4',\n",
       " 753: '855',\n",
       " 754: 'df0',\n",
       " 755: 'e14',\n",
       " 756: '2b0',\n",
       " 757: '2de',\n",
       " 758: 'd4e',\n",
       " 759: 'a9f',\n",
       " 760: 'aaf',\n",
       " 761: '8ff00',\n",
       " 762: '650',\n",
       " 763: 'a84',\n",
       " 764: 'a74',\n",
       " 765: '370',\n",
       " 766: 'a7b',\n",
       " 767: '154',\n",
       " 768: '39b',\n",
       " 769: '264',\n",
       " 770: 'a60',\n",
       " 771: '691',\n",
       " 772: '985',\n",
       " 773: '68e',\n",
       " 774: '120',\n",
       " 775: 'd5c',\n",
       " 776: 'd20',\n",
       " 777: 'dff00',\n",
       " 778: '88c',\n",
       " 779: '2c7',\n",
       " 780: 'a5b',\n",
       " 781: '6ff00',\n",
       " 782: '3dc',\n",
       " 783: 'a1c',\n",
       " 784: '5ae',\n",
       " 785: 'd3c',\n",
       " 786: '95e',\n",
       " 787: '6f5',\n",
       " 788: '25c',\n",
       " 789: '62c',\n",
       " 790: '315',\n",
       " 791: '94b',\n",
       " 792: '96b',\n",
       " 793: '14b',\n",
       " 794: '98f',\n",
       " 795: '1ef',\n",
       " 796: '6de',\n",
       " 797: '164',\n",
       " 798: 'd2d',\n",
       " 799: '30c',\n",
       " 800: '12b',\n",
       " 801: '35b',\n",
       " 802: '6cb',\n",
       " 803: '984',\n",
       " 804: 'a85',\n",
       " 805: '82a',\n",
       " 806: 'd14',\n",
       " 807: 'a07',\n",
       " 808: '1d4',\n",
       " 809: '61f',\n",
       " 810: '83f',\n",
       " 811: 'd75',\n",
       " 812: '13c',\n",
       " 813: '8db',\n",
       " 814: '107',\n",
       " 815: '13b',\n",
       " 816: '81b',\n",
       " 817: 'd5b',\n",
       " 818: '6fb',\n",
       " 819: 'aff00',\n",
       " 820: '923',\n",
       " 821: '29e',\n",
       " 822: '134',\n",
       " 823: '310',\n",
       " 824: '93e',\n",
       " 825: '88f',\n",
       " 826: '9f4',\n",
       " 827: '334',\n",
       " 828: '16e',\n",
       " 829: '9de',\n",
       " 830: 'd44',\n",
       " 831: 'a64',\n",
       " 832: '160',\n",
       " 833: 'aac',\n",
       " 834: 'a0f',\n",
       " 835: '65b',\n",
       " 836: '1d7',\n",
       " 837: '554',\n",
       " 838: '30b',\n",
       " 839: '94f',\n",
       " 840: '950',\n",
       " 841: '2c0',\n",
       " 842: '88b',\n",
       " 843: '9df',\n",
       " 844: '95c',\n",
       " 845: '8c4',\n",
       " 846: '620',\n",
       " 847: 'ab5',\n",
       " 848: '7be',\n",
       " 849: 'a3c',\n",
       " 850: '1e0',\n",
       " 851: 'aa5',\n",
       " 852: '14f',\n",
       " 853: '2df',\n",
       " 854: '823',\n",
       " 855: '24f',\n",
       " 856: '82b',\n",
       " 857: '8a8',\n",
       " 858: 'dee',\n",
       " 859: '1c7',\n",
       " 860: 'db0',\n",
       " 861: 'ac4',\n",
       " 862: 'a87',\n",
       " 863: '9f5',\n",
       " 864: 'ac7',\n",
       " 865: '6d7',\n",
       " 866: 'd0e',\n",
       " 867: '004',\n",
       " 868: '97b',\n",
       " 869: '3c5',\n",
       " 870: '8e7',\n",
       " 871: '294',\n",
       " 872: '1bb',\n",
       " 873: '3e4',\n",
       " 874: '1c4',\n",
       " 875: '6c7',\n",
       " 876: '890',\n",
       " 877: 'd17',\n",
       " 878: '80c',\n",
       " 879: '19b',\n",
       " 880: 'dbb',\n",
       " 881: '244',\n",
       " 882: '97e',\n",
       " 883: '2be',\n",
       " 884: '89b',\n",
       " 885: '97c',\n",
       " 886: 'aa4',\n",
       " 887: '157',\n",
       " 888: '85c',\n",
       " 889: '2ee',\n",
       " 890: '1f4',\n",
       " 891: 'a20',\n",
       " 892: '9cc',\n",
       " 893: '007',\n",
       " 894: '9e7',\n",
       " 895: 'a7c',\n",
       " 896: 'a2c',\n",
       " 897: '0000',\n",
       " 898: 'd2c',\n",
       " 899: '887',\n",
       " 900: '250',\n",
       " 901: '194',\n",
       " 902: '1ee',\n",
       " 903: '8b5',\n",
       " 904: '32c',\n",
       " 905: 'deb',\n",
       " 906: '13f',\n",
       " 907: '817',\n",
       " 908: '607',\n",
       " 909: '1a4',\n",
       " 910: 'a75',\n",
       " 911: '623',\n",
       " 912: 'a91',\n",
       " 913: 'ddb',\n",
       " 914: '2b7',\n",
       " 915: 'd2f',\n",
       " 916: '9ff00',\n",
       " 917: '30e',\n",
       " 918: 'd45',\n",
       " 919: '170',\n",
       " 920: '55e',\n",
       " 921: '891',\n",
       " 922: '195',\n",
       " 923: '240',\n",
       " 924: '67b',\n",
       " 925: '245',\n",
       " 926: '93b',\n",
       " 927: '335',\n",
       " 928: '1ac',\n",
       " 929: '2bc',\n",
       " 930: 'acf',\n",
       " 931: '12c',\n",
       " 932: '6c3',\n",
       " 933: '2cc',\n",
       " 934: '6a4',\n",
       " 935: '6b4',\n",
       " 936: 'a0c',\n",
       " 937: '3b7',\n",
       " 938: 'dd4',\n",
       " 939: 'a2e',\n",
       " 940: '3cb',\n",
       " 941: '704',\n",
       " 942: '2ec',\n",
       " 943: '6be',\n",
       " 944: '1d5',\n",
       " 945: '3d3',\n",
       " 946: '18f',\n",
       " 947: '1df',\n",
       " 948: '304',\n",
       " 949: '344',\n",
       " 950: '34e',\n",
       " 951: '9be',\n",
       " 952: '98b',\n",
       " 953: 'd6e',\n",
       " 954: 'a5c',\n",
       " 955: '265',\n",
       " 956: '6aa',\n",
       " 957: 'a94',\n",
       " 958: '8cc',\n",
       " 959: 'a15',\n",
       " 960: '37b',\n",
       " 961: 'd64',\n",
       " 962: '9fc',\n",
       " 963: '297',\n",
       " 964: '9a7',\n",
       " 965: '39c',\n",
       " 966: 'a04',\n",
       " 967: '82f',\n",
       " 968: '9af',\n",
       " 969: '9a3',\n",
       " 970: '844',\n",
       " 971: 'abb',\n",
       " 972: '8e3',\n",
       " 973: '2e0',\n",
       " 974: '85b',\n",
       " 975: '147',\n",
       " 976: '545',\n",
       " 977: '1e7',\n",
       " 978: '36f',\n",
       " 979: '1de',\n",
       " 980: '6bb',\n",
       " 981: '197',\n",
       " 982: 'a67',\n",
       " 983: 'ae0',\n",
       " 984: '8ac',\n",
       " 985: '31f',\n",
       " 986: 'a25',\n",
       " 987: '1eb',\n",
       " 988: '2a7',\n",
       " 989: '9bc',\n",
       " 990: '935',\n",
       " 991: '35f',\n",
       " 992: '555',\n",
       " 993: '99b',\n",
       " 994: '5c7',\n",
       " 995: '9d4',\n",
       " 996: '8a7',\n",
       " 997: '3bf',\n",
       " 998: '2e5',\n",
       " 999: '3c4',\n",
       " ...}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f63d381d927e0f98"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
